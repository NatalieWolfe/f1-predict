task = train
boosting_type = gbdt
objective = lambdarank
metric = ndcg

header = true

label_column = 0
group_column = 1
categorical_features = 2,3,4,5
# label_column = name:relevance_label
# group_column = name:race_id
# categorical_features = name:season,circuit_id,team_id,driver_id
# features = name:starting_position,q1_time_msec,q2_time_msec,q3_time_msec,driver_best_q_time_msec,gap_to_best_q_time_msec

# CSV column ordering (from writer.cc):
#
# relevance_label_column
# race_id_column
# circuit_id_column
# season_id_column
# team_id_column
# driver_id_column
# qual_spread_column
# starting_position_column
# q1_time_column
# q2_time_column
# q3_time_column
# driver_best_qual_time_column
# gap_to_best_qual_time_column
# qual_consistency_column
# driver_average_result_column
# driver_recent_average_result_column
# driver_career_stddev_column
# team_average_result_column
# team_recent_average_result_column

# train = "bazel-bin/training/training.csv"
# valid = "bazel-bin/training/tests.csv"

# Evaluate 1st place, podium, and top-5 accuracy
eval_at = 1,3,5
# Alternatives to try: 31 (default), 20, 40, 50
num_leaves = 31
# Consider shrinking to increase accuracy (also increase num_iterations).
learning_rate = 0.001
num_iterations = 500
early_stopping_round = 100

bagging_fraction = 0.8
bagging_freq = 1
feature_fraction = 0.8
lambda_l1 = 0.1
lambda_l2 = 0.1

# output_model = "f1_lambdarank_model.txt"

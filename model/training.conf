task = train
boosting_type = gbdt
objective = lambdarank
metric = ndcg

header = true

label_column = 0
group_column = 1
categorical_features = 2,3,4,5
label_gain = 0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30
# label_column = name:relevance_label
# group_column = name:race_id
# categorical_features = name:season,circuit_id,team_id,driver_id
# features = name:starting_position,q1_time_msec,q2_time_msec,q3_time_msec,driver_best_q_time_msec,gap_to_best_q_time_msec

# CSV column ordering (from writer.cc):
#
# relevance_label_column
# race_id_column
# circuit_id_column
# season_id_column
# team_id_column
# driver_id_column
# qual_spread_column
# starting_position_column
# q1_time_column
# q2_time_column
# q3_time_column
# driver_best_qual_time_column
# gap_to_best_qual_time_column
# qual_consistency_column
# driver_average_result_column
# driver_recent_average_result_column
# driver_career_stddev_column
# team_average_result_column
# team_recent_average_result_column

# train = "bazel-bin/training/training.csv"
# valid = "bazel-bin/training/tests.csv"

# Evaluate 1st place, podium, and top-5 accuracy
eval_at = 1,3,5
# Alternatives to try: 31 (default), 20, 40, 50
num_leaves = 31
# Consider shrinking to increase accuracy (also increase num_iterations).
learning_rate = 0.05
num_iterations = 500
early_stopping_round = 100

# output_model = "f1_lambdarank_model.txt"
